{"componentChunkName":"component---src-pages-day-2-ops-readme-mdx","path":"/day2-ops/README/","result":{"pageContext":{"frontmatter":{"description":"Day 2 Ops / README","title":"Day 2 Ops / README"},"relativePagePath":"/day2-ops/README.mdx","titleType":"append","MdxNode":{"id":"6537881d-fc75-5ec9-8a03-eec813535aff","children":[],"parent":"43d4c7fe-a7ea-5b3e-b40d-d28192bfaa72","internal":{"content":"# Install a Prometheus instance to monitor an Open Liberty application on OCP 4.3\n\nWe will use the Prometheus Operator provided by OCP to install and configure Prometheus into a new namespace (project). We will then install a test application using the Open Liberty Operator. The application will provide a metrics endpoint for Prometheus to gather statistics.   \n\n\n## 1. Test app\nUse a simple Microprofile metrics demo application from Open Liberty guides\nhttps://openliberty.io/guides/microprofile-metrics.html\n\nThis app is used 'as is' (although we disabled the default security for now to make it simpler to play with).Â  You can pull the docker image from the hub to run it locally.\n```\ndocker run -p 9080:9080 yarod/microprofile-monitoring\n```\n\n## 2. Install and configure Prometheus\n\nOCP provides a monitoring stack (Prometheus + Grafana) but up to now has not allowed it to be used for user workload monitoring.\n\nAs there is a Liberty monitoring dashboard for Grafana designed to visualize Liberty-specific Microprofile metrics, we will use our own instance of Prometheus to gather the metrics and as a target for a new Grafana instance to display the Liberty dashboard.  \n\n### 2.1 Create the required Service IDs, ClusterRoles, and ClusterRoleBindings\n\nThis scenario generally follows Kabanero documentation and installs a separate Prometheus instance.\n\nhttps://kabanero.io/guides/app-monitoring-ocp4.2\n\n#### 2.1.1  Create a new Project/Namespace\n\nTo facilitate security and isolation we want to run Prometheus in its own namespace.  To achieve this we create a new project/namespace for it.  For our example we will call it \"prometheus-test\". You must be logged into the OCP system and able to use the Openshift CLI (oc) commands to complete this exercise. We use the \"test\" suffix throughout our examples but in production you would likely use some other suffix that identifies your applications.\n~~~\noc new-project prometheus-test\n~~~\n\n#### 2.1.2 Service Accounts\n\nBoth the *Prometheus Operator* and *Prometheus* itself must have an identity (account) when they are run.  Each of these programs requires specific authorizations in order to function. In a Role Based Authentication System (RBAC) like OpenShift these accounts are called **Service Accounts**. Roles with cluster wide access are called **ClusterRoles**. To connect an account with a clusterrole you use a **ClusterRoleBinding**.  Note that you will require admin authority on the cluster to create, alter, or delete cluster roles and bindings.\n\nThe service account for the *Prometheus Operator* is named **prometheus-operator**. To create it copy the file PromOperServiceAccount.yaml and issue the command:\n\n~~~\noc apply -f PromOperServiceAccount.yaml\n~~~\n\nDo the same for the next two:\n\nThe service acccount for *Prometheus* itself:\n\n~~~\noc apply -f PrometheusServiceAccount.yaml\n~~~\nAdd the 2nd service account that Prometheus expects:\n~~~\noc apply -f k8sServiceAccount.yaml\n~~~\nconfirm via:\n~~~\noc get serviceaccount\n~~~\nand you should see the three new service accounts (along with those kubernetes creates for each namespace):\n\n~~~\nNAME                  SECRETS   AGE\nbuilder               2         7m49s\ndefault               2         7m49s\ndeployer              2         7m49s\nprometheus            2         115s\nprometheus-k8s        2         4s\nprometheus-operator   2         3m30s\n~~~\n\n\n#### 2.1.3 Create the Cluster Roles\n\nAs we are running on an OpenShift system there are cluster roles already for those service IDs. However, those cluster roles were defined for the *openshift-monitoring* namespace. They may not match those we need for our instance of prometheus.  Additionally, any changes to those cluster roles by cluster administrators can potentially have unknown consequences for our instance.\n\n Therefore, we will create our own versions of the cluster roles (note: if you wish to reuse the existing cluster roles you must still create our prometheus-k8s-test cluster role if you want Prometheus to monitor an application outside its own namespace as the default prometheus-k8s cluster role does not have that ability).\n\nYou are free to replace the *test* suffix with something more meaningful but then you must edit the cluster role bindings yaml files  to pick up the new object names.\n\n prometheus-operator-test cluster role:\n ~~~\n oc apply -f promoprole.yaml\n ~~~\n\n prometheus-test cluster role:\n ~~~\noc apply -f promrole.yaml\n ~~~\n\n prometheus-k8s-test cluster role:\n\n ~~~\n oc apply -f prometheus-k8s-custom.yaml\n ~~~\n Note that the prometheus-k8s-test cluster role is a merge of the default OCP permissions for the prometheus-k8s account and those recommended by the *Prometheus Operator* documentation.\n\n Before granting users access to the Prometheus namespace it is recommended that you review and understand the permissions for all three service IDs.\n\n Now you can confirm the ClusterRoles you have created via:\n\n ~~~\n oc get clusterrole\n ~~~\n This will display all the ClusterRoles for the system.\n Or confirm them individually:\n ~~~\n oc get clusterrole/prometheus-test\n\n oc get clusterrole/prometheus-operator-test\n\n oc get clusterrole/prometheus-k8s-test\n~~~\nTo view the contents of an individual clusterrole:\n~~~\noc describe clusterrole/prometheus-test\n~~~\nexample output:\n~~~\nName:         prometheus-test\nLabels:       <none>\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration:\n                {\"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\",\"kind\":\"ClusterRole\",\"metadata\":{\"annotations\":{},\"name\":\"prometheus-test\"},\"rules\":[{\"a...\nPolicyRule:\n  Resources   Non-Resource URLs  Resource Names  Verbs\n  ---------   -----------------  --------------  -----\n  endpoints   []                 []              [get list watch]\n  nodes       []                 []              [get list watch]\n  pods        []                 []              [get list watch]\n  services    []                 []              [get list watch]\n              [/metrics]         []              [get]\n  configmaps  []                 []              [get]\n~~~\n\n\n\n\n#### 2.1.4 Bind the Service Accounts to their Cluster Roles ###\n\n\nThis is done via a *ClusterRoleBinding*.\n\nprometheus:\n~~~\noc apply -f prombinding.yaml\n~~~\n\nprometheus-operator:\n~~~\noc apply -f promopbinding.yaml\n~~~\n\nprometheus-k8s:\n\n~~~\noc apply -f customrolebindingK8s.yaml\n~~~\n\n\n\nThese are based on information in the Github site for the Prometheus Operator. You can refer to it at:  https://github.com/coreos/prometheus-operator/\n\n### 2.2 Create a Prometheus instance\n\n#### 2.2.1 Install the Prometheus Operator\n\nFrom the IBM Cloud Dashboard select the project (in our example: prometheus-test). Then open the *Operators* section and click on *Installed Operators*:\n\n![Installed Operators](/Images/InitialInstalledOperators.png)\n\nOur system has only the Open Liberty Operator installed so we must add the Prometheus Operator to it.\n\nSelect the *OperatorHub* option:\n\n![InitialHub](/Images/InitialHub.png)\n\nPut the name prometheus into the *Filter by keyword* area:\n\n![HubProm](/Images/HubProm.png)\n\nClick on the *Prometheus Operator* and answer *Continue* to the warning screen. This will bring up the *Install* option:\n\n![InstallOption](/Images/InstallOption.png)\n\nThis is the Prometheus Operator Subscription Screen.\n\n![PromSub](/Images/OperatorSubscribe.png)\n\nClick on *Subscribe* at the bottom on the screen.\n\n![Subselect](/Images/OpSubOrCancel.png)\n\nNow examine the *Installed Operators* and you will see that you now have the *Prometheus Operator* available:\n\n![PromOp](/Images/PromOpInstalled.png)\n\n#### 2.2.2 Install & Configure a Prometheus Instance\nFrom the *Installed Operators* click on the *Prometheus Operator*\n\n![PromOp](/Images/PromOpInstalled.png)\n\nThis will bring up the *Prometheus Operator* screen. Click on *Create Instance*\n\n![CreateProm](/Images/PromCreate.png)\n\nand you will see the *Create* selection\n\n![PromConfig](/Images/PromConfig.png)\n\n\nCopy the contents of the *PromConfigNames.yaml* file into the selection  area of the screen and click on *Create*. This will instantiate prometheus in your namespace (this yaml will need to be customized for a production installation, for example to control storage usage, you can find the options at https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md).\n\nVerify the Prometheus Service is running:\n~~~\noc get svc -n prometheus-test\n~~~\n\n~~~\nNAME                  TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nprometheus-operated   ClusterIP   None         <none>        9090/TCP   4m54s\n~~~\n\nExpose the *prometheus-operated* service externally:\n~~~\noc expose svc/prometheus-operated -n prometheus-test\n~~~\n~~~\n\n13. Confirm the external route:\n~~~\noc get routes -n prometheus-test\n~~~\nroute.route.openshift.io/prometheus-operated exposed\n~~~\n\nNow go to your project dashboard page and scroll down to the Deployments area.\n\n![PromDeploy](/Images/PromDeployment.png)\n\nClick on the *Route* option\n\n![PromRoute](Images/PromRoute.png)\n\nNow click on the URL under the *Location* header to open a new window for the Prometheus console\n\n![Promcon](/Images/PromConsole.png)\n\nPrometheus has been installed and is running although as yet it hasn't found any applications to monitor.\n\n\n## 3 Deploy an app via the Open Liberty Operator\n\nSo far we have not created any monitoring targets for Prometheus.  Note that the Prometheus we installed earlier is managed by the Prometheus operator, and the latter will monitor for ServiceMonitor CRD instances to identify monitoring targets.  \n\n### 3.1 Create a new project for the test application\n\nCreate a new project to hold your application. Note that we are now working in the *dev-test* namespace.\n~~~\noc new-project dev-test\n~~~\n\n### 3.2 Install the Open Liberty Operator\n\nInstall the Open Liberty operator from the Operator Hub or Installed Operators as you did for the Prometheus Operator.\n\nThe documentation for the *Open Liberty Operator* can be found at:\n\nhttps://github.com/OpenLiberty/open-liberty-operator/blob/master/doc/user-guide.md\n\n\n![Libhub](/Images/LibertyOpHub.png)\n\n\nShould you need to debug the  Open Liberty operator deployment, you can watch the operator logs that show the deployment activity.\n~~~\n oc get pods -A |grep liberty\n ~~~\n\n The response will look like:\n ~~~\n openshift-operators  open-liberty-operator-847d9d76c4-gt24z 1/1\n ~~~\n\nThen take the name of the open-liberty-operator pod from the openshift-operators namespace and plug it into the logs command:\n\n\n~~~\n oc logs  open-liberty-operator-847d9d76c4-gt24z -f -n openshift-operators\n~~~\nNote: the response is not shown here because it is extensive. (ctl-c to end the output in a linux terminal)\n\n### 3.3 Create an Open Liberty Application\n\nOnce available the *Installed Operators* option in your project will show it.\n\n![libopin](/Images/LibOpInstalled.png)\n\nSelect the Open Liberty Operator to open it\n\n![opliberty](/Images/OpLiberty.png)\n\nIn the *Open Liberty Application* area select *Create Instance*\n\n![libcreate](/Images/LibertyCreate.png)\n\nNow copy the contents of the *openl.yaml* file into the screen for the new application and click on *Create*\n\n![openlyaml](/Images/openlyaml.png)\n\nNow go back to the dev-test project dashboard and scroll down to the deployments selection to view your deployed application\n\n![openldeply](/Images/openldeployed.png)\n\nThe *Route* will point to the application\n\n![olroute](/Images/olroute.png)\n\nClick on the URL to view the application\n\n![olapp](/Images/olapp.png)\n\nIf you append */metrics* to the URL in your browser you will see the metrics that the application provides for Monitoring\n\n![olmetrics](/Images/olmetrics.png)\n\n### 3.4 Create a ServiceMonitor so Prometheus can find the application\n\nAlthough the Open Liberty Operator has created a ServiceMonitor object in the dev-test namespace; Prometheus (in this implementation) requires it to be in its own namespace. So we must create it.  \n\nSwitch back to the *prometheus-test* project and go to the *Installed Operators* again and select the *Prometheus Operator*. Scroll down to find the *Service Monitor* section.\n\n![createSM](/Images/smcreate.png)\n\nClick on *Create Instance* and copy/paste the contents of the *servicemonitor-test.yaml* file into the create Screen then click on *Create* to generate the ServiceMonitor object\n\n![smcontents](/Images/smcontents.png)\n\nYou can view the installed servicemonitors from the Prometheus Operator screens or via oc commands\n\n~~~\noc project prometheus-test\n~~~\n\n~~~\noc get servicemonitor\n~~~\n\nreturns\n\n~~~\nNAME   AGE\ntest   28s\n~~~\n\n~~~\noc describe servicemonitor/test\n~~~\n\nreturns the contents of our new *test* ServiceMonitor\n\n~~~\nName:         test\nNamespace:    prometheus-test\nLabels:       k8s-app=prometheus\n              name=test\nAnnotations:  <none>\nAPI Version:  monitoring.coreos.com/v1\nKind:         ServiceMonitor\nMetadata:\n  Creation Timestamp:  2020-04-08T00:15:06Z\n  Generation:          1\n  Resource Version:    7626170\n  Self Link:           /apis/monitoring.coreos.com/v1/namespaces/prometheus-test/servicemonitors/test\n  UID:                 59784c39-3a8d-4c9e-8e11-57a26a0487e0\nSpec:\n  Endpoints:\n    Interval:  30s\n    Bearer Token Secret:\n      Key:  \n    Port:   9080-tcp\n  Namespace Selector:\n    Match Names:\n      dev-test\n  Selector:\n    Match Labels:\n      k8s-app:  \nEvents:         <none>\n~~~\n\n## 4 Use Prometheus to view the application Metrics\n\nOur Prometheus instance should now be able to find our application via the *test servicemonitor* and it will begin to scrape the metrics every 30 seconds as we specified.\n\nReturn to the *Route* section of the *prometheus-test* dashboard\n\n![promroute2](/Images/Promroute2.png)\n\n Click on the URL to return to the Prometheus console. Select the drop down in the *-insert metric at cursor -* box, click on any metric and then click on the *Execute* box. This will display that metric\n\n![PromConsole2](/Images/PromConsoleMetrics.png)\n\nNow select the *Status* drop down from the header bar and click on *Targets*\n\n![targets](/Images/PromTargets.png)\n\nYou can see Prometheus is monitoring our application.  \n","type":"Mdx","contentDigest":"89646154a418ef680152d12d95e2c090","counter":436,"owner":"gatsby-plugin-mdx"},"frontmatter":{"description":"Day 2 Ops / README","title":"Day 2 Ops / README"},"exports":{},"rawBody":"# Install a Prometheus instance to monitor an Open Liberty application on OCP 4.3\n\nWe will use the Prometheus Operator provided by OCP to install and configure Prometheus into a new namespace (project). We will then install a test application using the Open Liberty Operator. The application will provide a metrics endpoint for Prometheus to gather statistics.   \n\n\n## 1. Test app\nUse a simple Microprofile metrics demo application from Open Liberty guides\nhttps://openliberty.io/guides/microprofile-metrics.html\n\nThis app is used 'as is' (although we disabled the default security for now to make it simpler to play with).Â  You can pull the docker image from the hub to run it locally.\n```\ndocker run -p 9080:9080 yarod/microprofile-monitoring\n```\n\n## 2. Install and configure Prometheus\n\nOCP provides a monitoring stack (Prometheus + Grafana) but up to now has not allowed it to be used for user workload monitoring.\n\nAs there is a Liberty monitoring dashboard for Grafana designed to visualize Liberty-specific Microprofile metrics, we will use our own instance of Prometheus to gather the metrics and as a target for a new Grafana instance to display the Liberty dashboard.  \n\n### 2.1 Create the required Service IDs, ClusterRoles, and ClusterRoleBindings\n\nThis scenario generally follows Kabanero documentation and installs a separate Prometheus instance.\n\nhttps://kabanero.io/guides/app-monitoring-ocp4.2\n\n#### 2.1.1  Create a new Project/Namespace\n\nTo facilitate security and isolation we want to run Prometheus in its own namespace.  To achieve this we create a new project/namespace for it.  For our example we will call it \"prometheus-test\". You must be logged into the OCP system and able to use the Openshift CLI (oc) commands to complete this exercise. We use the \"test\" suffix throughout our examples but in production you would likely use some other suffix that identifies your applications.\n~~~\noc new-project prometheus-test\n~~~\n\n#### 2.1.2 Service Accounts\n\nBoth the *Prometheus Operator* and *Prometheus* itself must have an identity (account) when they are run.  Each of these programs requires specific authorizations in order to function. In a Role Based Authentication System (RBAC) like OpenShift these accounts are called **Service Accounts**. Roles with cluster wide access are called **ClusterRoles**. To connect an account with a clusterrole you use a **ClusterRoleBinding**.  Note that you will require admin authority on the cluster to create, alter, or delete cluster roles and bindings.\n\nThe service account for the *Prometheus Operator* is named **prometheus-operator**. To create it copy the file PromOperServiceAccount.yaml and issue the command:\n\n~~~\noc apply -f PromOperServiceAccount.yaml\n~~~\n\nDo the same for the next two:\n\nThe service acccount for *Prometheus* itself:\n\n~~~\noc apply -f PrometheusServiceAccount.yaml\n~~~\nAdd the 2nd service account that Prometheus expects:\n~~~\noc apply -f k8sServiceAccount.yaml\n~~~\nconfirm via:\n~~~\noc get serviceaccount\n~~~\nand you should see the three new service accounts (along with those kubernetes creates for each namespace):\n\n~~~\nNAME                  SECRETS   AGE\nbuilder               2         7m49s\ndefault               2         7m49s\ndeployer              2         7m49s\nprometheus            2         115s\nprometheus-k8s        2         4s\nprometheus-operator   2         3m30s\n~~~\n\n\n#### 2.1.3 Create the Cluster Roles\n\nAs we are running on an OpenShift system there are cluster roles already for those service IDs. However, those cluster roles were defined for the *openshift-monitoring* namespace. They may not match those we need for our instance of prometheus.  Additionally, any changes to those cluster roles by cluster administrators can potentially have unknown consequences for our instance.\n\n Therefore, we will create our own versions of the cluster roles (note: if you wish to reuse the existing cluster roles you must still create our prometheus-k8s-test cluster role if you want Prometheus to monitor an application outside its own namespace as the default prometheus-k8s cluster role does not have that ability).\n\nYou are free to replace the *test* suffix with something more meaningful but then you must edit the cluster role bindings yaml files  to pick up the new object names.\n\n prometheus-operator-test cluster role:\n ~~~\n oc apply -f promoprole.yaml\n ~~~\n\n prometheus-test cluster role:\n ~~~\noc apply -f promrole.yaml\n ~~~\n\n prometheus-k8s-test cluster role:\n\n ~~~\n oc apply -f prometheus-k8s-custom.yaml\n ~~~\n Note that the prometheus-k8s-test cluster role is a merge of the default OCP permissions for the prometheus-k8s account and those recommended by the *Prometheus Operator* documentation.\n\n Before granting users access to the Prometheus namespace it is recommended that you review and understand the permissions for all three service IDs.\n\n Now you can confirm the ClusterRoles you have created via:\n\n ~~~\n oc get clusterrole\n ~~~\n This will display all the ClusterRoles for the system.\n Or confirm them individually:\n ~~~\n oc get clusterrole/prometheus-test\n\n oc get clusterrole/prometheus-operator-test\n\n oc get clusterrole/prometheus-k8s-test\n~~~\nTo view the contents of an individual clusterrole:\n~~~\noc describe clusterrole/prometheus-test\n~~~\nexample output:\n~~~\nName:         prometheus-test\nLabels:       <none>\nAnnotations:  kubectl.kubernetes.io/last-applied-configuration:\n                {\"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\",\"kind\":\"ClusterRole\",\"metadata\":{\"annotations\":{},\"name\":\"prometheus-test\"},\"rules\":[{\"a...\nPolicyRule:\n  Resources   Non-Resource URLs  Resource Names  Verbs\n  ---------   -----------------  --------------  -----\n  endpoints   []                 []              [get list watch]\n  nodes       []                 []              [get list watch]\n  pods        []                 []              [get list watch]\n  services    []                 []              [get list watch]\n              [/metrics]         []              [get]\n  configmaps  []                 []              [get]\n~~~\n\n\n\n\n#### 2.1.4 Bind the Service Accounts to their Cluster Roles ###\n\n\nThis is done via a *ClusterRoleBinding*.\n\nprometheus:\n~~~\noc apply -f prombinding.yaml\n~~~\n\nprometheus-operator:\n~~~\noc apply -f promopbinding.yaml\n~~~\n\nprometheus-k8s:\n\n~~~\noc apply -f customrolebindingK8s.yaml\n~~~\n\n\n\nThese are based on information in the Github site for the Prometheus Operator. You can refer to it at:  https://github.com/coreos/prometheus-operator/\n\n### 2.2 Create a Prometheus instance\n\n#### 2.2.1 Install the Prometheus Operator\n\nFrom the IBM Cloud Dashboard select the project (in our example: prometheus-test). Then open the *Operators* section and click on *Installed Operators*:\n\n![Installed Operators](/Images/InitialInstalledOperators.png)\n\nOur system has only the Open Liberty Operator installed so we must add the Prometheus Operator to it.\n\nSelect the *OperatorHub* option:\n\n![InitialHub](/Images/InitialHub.png)\n\nPut the name prometheus into the *Filter by keyword* area:\n\n![HubProm](/Images/HubProm.png)\n\nClick on the *Prometheus Operator* and answer *Continue* to the warning screen. This will bring up the *Install* option:\n\n![InstallOption](/Images/InstallOption.png)\n\nThis is the Prometheus Operator Subscription Screen.\n\n![PromSub](/Images/OperatorSubscribe.png)\n\nClick on *Subscribe* at the bottom on the screen.\n\n![Subselect](/Images/OpSubOrCancel.png)\n\nNow examine the *Installed Operators* and you will see that you now have the *Prometheus Operator* available:\n\n![PromOp](/Images/PromOpInstalled.png)\n\n#### 2.2.2 Install & Configure a Prometheus Instance\nFrom the *Installed Operators* click on the *Prometheus Operator*\n\n![PromOp](/Images/PromOpInstalled.png)\n\nThis will bring up the *Prometheus Operator* screen. Click on *Create Instance*\n\n![CreateProm](/Images/PromCreate.png)\n\nand you will see the *Create* selection\n\n![PromConfig](/Images/PromConfig.png)\n\n\nCopy the contents of the *PromConfigNames.yaml* file into the selection  area of the screen and click on *Create*. This will instantiate prometheus in your namespace (this yaml will need to be customized for a production installation, for example to control storage usage, you can find the options at https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md).\n\nVerify the Prometheus Service is running:\n~~~\noc get svc -n prometheus-test\n~~~\n\n~~~\nNAME                  TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nprometheus-operated   ClusterIP   None         <none>        9090/TCP   4m54s\n~~~\n\nExpose the *prometheus-operated* service externally:\n~~~\noc expose svc/prometheus-operated -n prometheus-test\n~~~\n~~~\n\n13. Confirm the external route:\n~~~\noc get routes -n prometheus-test\n~~~\nroute.route.openshift.io/prometheus-operated exposed\n~~~\n\nNow go to your project dashboard page and scroll down to the Deployments area.\n\n![PromDeploy](/Images/PromDeployment.png)\n\nClick on the *Route* option\n\n![PromRoute](Images/PromRoute.png)\n\nNow click on the URL under the *Location* header to open a new window for the Prometheus console\n\n![Promcon](/Images/PromConsole.png)\n\nPrometheus has been installed and is running although as yet it hasn't found any applications to monitor.\n\n\n## 3 Deploy an app via the Open Liberty Operator\n\nSo far we have not created any monitoring targets for Prometheus.  Note that the Prometheus we installed earlier is managed by the Prometheus operator, and the latter will monitor for ServiceMonitor CRD instances to identify monitoring targets.  \n\n### 3.1 Create a new project for the test application\n\nCreate a new project to hold your application. Note that we are now working in the *dev-test* namespace.\n~~~\noc new-project dev-test\n~~~\n\n### 3.2 Install the Open Liberty Operator\n\nInstall the Open Liberty operator from the Operator Hub or Installed Operators as you did for the Prometheus Operator.\n\nThe documentation for the *Open Liberty Operator* can be found at:\n\nhttps://github.com/OpenLiberty/open-liberty-operator/blob/master/doc/user-guide.md\n\n\n![Libhub](/Images/LibertyOpHub.png)\n\n\nShould you need to debug the  Open Liberty operator deployment, you can watch the operator logs that show the deployment activity.\n~~~\n oc get pods -A |grep liberty\n ~~~\n\n The response will look like:\n ~~~\n openshift-operators  open-liberty-operator-847d9d76c4-gt24z 1/1\n ~~~\n\nThen take the name of the open-liberty-operator pod from the openshift-operators namespace and plug it into the logs command:\n\n\n~~~\n oc logs  open-liberty-operator-847d9d76c4-gt24z -f -n openshift-operators\n~~~\nNote: the response is not shown here because it is extensive. (ctl-c to end the output in a linux terminal)\n\n### 3.3 Create an Open Liberty Application\n\nOnce available the *Installed Operators* option in your project will show it.\n\n![libopin](/Images/LibOpInstalled.png)\n\nSelect the Open Liberty Operator to open it\n\n![opliberty](/Images/OpLiberty.png)\n\nIn the *Open Liberty Application* area select *Create Instance*\n\n![libcreate](/Images/LibertyCreate.png)\n\nNow copy the contents of the *openl.yaml* file into the screen for the new application and click on *Create*\n\n![openlyaml](/Images/openlyaml.png)\n\nNow go back to the dev-test project dashboard and scroll down to the deployments selection to view your deployed application\n\n![openldeply](/Images/openldeployed.png)\n\nThe *Route* will point to the application\n\n![olroute](/Images/olroute.png)\n\nClick on the URL to view the application\n\n![olapp](/Images/olapp.png)\n\nIf you append */metrics* to the URL in your browser you will see the metrics that the application provides for Monitoring\n\n![olmetrics](/Images/olmetrics.png)\n\n### 3.4 Create a ServiceMonitor so Prometheus can find the application\n\nAlthough the Open Liberty Operator has created a ServiceMonitor object in the dev-test namespace; Prometheus (in this implementation) requires it to be in its own namespace. So we must create it.  \n\nSwitch back to the *prometheus-test* project and go to the *Installed Operators* again and select the *Prometheus Operator*. Scroll down to find the *Service Monitor* section.\n\n![createSM](/Images/smcreate.png)\n\nClick on *Create Instance* and copy/paste the contents of the *servicemonitor-test.yaml* file into the create Screen then click on *Create* to generate the ServiceMonitor object\n\n![smcontents](/Images/smcontents.png)\n\nYou can view the installed servicemonitors from the Prometheus Operator screens or via oc commands\n\n~~~\noc project prometheus-test\n~~~\n\n~~~\noc get servicemonitor\n~~~\n\nreturns\n\n~~~\nNAME   AGE\ntest   28s\n~~~\n\n~~~\noc describe servicemonitor/test\n~~~\n\nreturns the contents of our new *test* ServiceMonitor\n\n~~~\nName:         test\nNamespace:    prometheus-test\nLabels:       k8s-app=prometheus\n              name=test\nAnnotations:  <none>\nAPI Version:  monitoring.coreos.com/v1\nKind:         ServiceMonitor\nMetadata:\n  Creation Timestamp:  2020-04-08T00:15:06Z\n  Generation:          1\n  Resource Version:    7626170\n  Self Link:           /apis/monitoring.coreos.com/v1/namespaces/prometheus-test/servicemonitors/test\n  UID:                 59784c39-3a8d-4c9e-8e11-57a26a0487e0\nSpec:\n  Endpoints:\n    Interval:  30s\n    Bearer Token Secret:\n      Key:  \n    Port:   9080-tcp\n  Namespace Selector:\n    Match Names:\n      dev-test\n  Selector:\n    Match Labels:\n      k8s-app:  \nEvents:         <none>\n~~~\n\n## 4 Use Prometheus to view the application Metrics\n\nOur Prometheus instance should now be able to find our application via the *test servicemonitor* and it will begin to scrape the metrics every 30 seconds as we specified.\n\nReturn to the *Route* section of the *prometheus-test* dashboard\n\n![promroute2](/Images/Promroute2.png)\n\n Click on the URL to return to the Prometheus console. Select the drop down in the *-insert metric at cursor -* box, click on any metric and then click on the *Execute* box. This will display that metric\n\n![PromConsole2](/Images/PromConsoleMetrics.png)\n\nNow select the *Status* drop down from the header bar and click on *Targets*\n\n![targets](/Images/PromTargets.png)\n\nYou can see Prometheus is monitoring our application.  \n","fileAbsolutePath":"/home/runner/work/modernization-playbook/modernization-playbook/src/pages/day2-ops/README.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}