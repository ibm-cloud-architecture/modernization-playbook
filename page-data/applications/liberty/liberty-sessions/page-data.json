{"componentChunkName":"component---src-pages-applications-liberty-liberty-sessions-mdx","path":"/applications/liberty/liberty-sessions/","result":{"pageContext":{"frontmatter":{"title":"Managing session state","description":"Best practices and techniques to managing session state in an application when modernizing"},"relativePagePath":"/applications/liberty/liberty-sessions.mdx","titleType":"append","MdxNode":{"id":"991c4602-9bb2-51de-9267-f501d8f9e3f0","children":[],"parent":"37d95b05-eb6d-589d-87be-2e4451c2ed53","internal":{"content":"---\ntitle: Managing session state\ndescription: Best practices and techniques to managing session state in an application when modernizing\n---\n\nexport const Title = () => (\n  <span>\nModernizing runtimes with Liberty <br/> <h2>Managing session state in an application</h2>\n  </span>\n);\n\n<PageDescription>\nBest practices and techniques to managing session state in an application when modernizing\n</PageDescription>\n\n\n\n\n# Session state management considerations\n\nOne of the [12 factor](https://12factor.net/) principles is that applications and services should be stateless. Many existing monolith applications rely heavily on HTTP sessions. When you are modernizing your application it is a good practice to rearchitect them to be stateless, but some times it is not an easy task and you need to postpone it for a while.\n\nThere are following methods that you can use in container world to handle session state, depending on your requirements:\n- session affinity\n- session replication using caching mechanisms\n- session persistence using database\n\n\n## Session affinity\n\nSession affinity is the simplest mechanism for maintaining session state. Using this mechanism, requests from the same session will be routed to the same POD. Selecting this solution, you have to be aware that PODs are not like on-premise servers, they may be restarted much often, in many circumstances. So this solution might be applicable to applications that need relatively short lived sessions, and can tolerate loosing session data.\n\n### Deploying with session affinity\n\nSession affinity on OpenShift utilizes Route configuration. You don't have to make any changes in your application code or during building application container, all is done on deployment time.\n\nIn this example, a simple application has been deployed to WebSphere Liberty that displays the contents of the *session* and allows updates to be made to a *counter* that is stored in the session as well as displaying the name of the *pod* that services the request. There are multiple instances of the application pod running:\n\n```\noc get pods | grep sample\nsession-sample-1-bzpd7   1/1     Running   0          10d\nsession-sample-1-dnpsg   1/1     Running   0          10d\nsession-sample-1-g5c8k   1/1     Running   0          21d\n```\n\nOn the first request, the output is as shown below with the name of the pod an empty session. Since this is first request, currently there is no data in the session. You can also see name of the pod that runs the application.\n\n![app1](images/sessions-run/oc-app1.jpg)\n\nAfter a few requests to the app, using the link or reloading the page, the the counter value that is taken from the session increases as shwon below. Note that the requests are routed to the same pod. Session affinity is working.\n\n![app2](images/sessions-run/oc-app2.jpg)\n\nIf the pod is then deleted it will be replaced by OpenShift:\n\n```\noc delete pod session-sample-1-g5c8k\npod \"session-sample-1-g5c8k\" deleted\n\noc get pods | grep sample\nsession-sample-1-5hzd5   1/1     Running   0          3m\nsession-sample-1-7w8gh   1/1     Running   0          23s\nsession-sample-1-dnpsg   1/1     Running   0          10d\n```\n\nIf an attempt is then made to refresh the browser page or increment the count then the request is routed to the different pod and session state is lost:\n\n![app4](images/sessions-run/oc-app3.jpg)\n\nIf you want to ensure that session state is not lost when pod is removed, you will need to configure session caching or session persistence.\n\n## Session replication using caching mechanisms\nVery often session affinity is not enough for application requirements and you need session state to be preserved in case of pod failure. One of the mechanisms that could be used is *session caching*. WebSphere Liberty has a `sessionCache-1.0` feature which provides distributed in-memory HttpSession caching. The `sessionCache-1.0` feature builds on top of an existing technology called [JCache (JSR 107)](https://www.jcp.org/en/jsr/detail?id=107), which offers a standardized distributed in-memory caching API.\n\nThe `sessionCache-1.0` feature does not include a JCache implementation, so you need to pick one and reference it as a `<library>` in your `server.xml`. WebSphere/Open Liberty supports the following JCache implementations:\n- Hazelcast\n- WebSphere Extreme Scale\n- Infinispan\n- Ehcache\n\n### Prepare DockerFile with caching configuration\nThis article shows how to enable [Hazelcast In-Memory Data Grid](https://hazelcast.org/), as it is easily available in many private cloud solutions.\n\nEnabling Hazelcast session caching retrieves the Hazelcast client libraries from the [hazelcast/hazelcast](https://hub.docker.com/r/hazelcast/hazelcast/) Docker image, configures Hazelcast by copying a sample hazelcast.xml, and configures the Liberty server feature sessionCache-1.0 by including the XML snippet hazelcast-sessioncache.xml. By default, the Hazelcast Discovery Plugin for Kubernetes will auto-discover its peers within the same Kubernetes namespace. To enable this functionality, the Docker image author can include the following Dockerfile snippet, and choose from either client-server or embedded topology.\n\nModify your current Dockerfile with the following lines:\n\n```dockerfile\n### Hazelcast Session Caching ###\n# Copy the Hazelcast libraries from the Hazelcast Docker image - paths for WebSphere Liberty\nCOPY --from=hazelcast/hazelcast --chown=1001:0 /opt/hazelcast/lib/*.jar /opt/ibm/wlp/usr/shared/resources/hazelcast/\n\n# Copy the Hazelcast libraries from the Hazelcast Docker image - paths for Open Liberty\n# COPY --from=hazelcast/hazelcast --chown=1001:0 /opt/hazelcast/lib/*.jar /opt/ol/wlp/usr/shared/resources/hazelcast/\n\n# Instruct configure.sh to copy the client topology hazelcast.xml\nARG HZ_SESSION_CACHE=client\n\n# Instruct configure.sh to copy the embedded topology hazelcast.xml and set the required system property\n#ARG HZ_SESSION_CACHE=embedded\n#ENV JAVA_TOOL_OPTIONS=\"-Dhazelcast.jcache.provider.type=server ${JAVA_TOOL_OPTIONS}\"\n\n## This script will add the requested XML snippets and grow image to be fit-for-purpose\nRUN configure.sh\n```\n\n### Deploy Hazelcast in OpenShift\nHazelcast can be used in OpenShift. By default it requires paid version - Hazelcast Enterprise. But you can use also free version [Hazelcast OpenShift Origin](https://github.com/hazelcast/hazelcast-code-samples/blob/master/hazelcast-integration/openshift/hazelcast-cluster/hazelcast-openshift-origin). See details on this page [Hazelcast for OpenShift](https://github.com/hazelcast/hazelcast-code-samples/tree/master/hazelcast-integration/openshift)\n\nFor this article you will use Hazelcast OpenShift Origin.\n\n#### Deploy Hazelcast cluster\nEasiest way to deploy Hazelcast is to use [hazelcast.yaml](https://github.com/hazelcast/hazelcast-code-samples/blob/master/hazelcast-integration/openshift/hazelcast-cluster/hazelcast-openshift-origin/hazelcast.yaml) file provided by Hazelcast\n\nCreate project for Hazelcast cluster\n\n`$ oc new-project appmod-hazelcast`\n\nDeploy Hazelcast cluster\n\n`$ oc new-app -f hazelcast.yaml -p NAMESPACE=appmod-hazelcast`\n\nCheck the status of deployed cluster\n\n```\n$ oc get all\nNAME              READY     STATUS              RESTARTS   AGE\npod/hazelcast-0   1/1       Running             0          1m\npod/hazelcast-1   1/1       Running             0          42s\npod/hazelcast-2   0/1       ContainerCreating   0          2s\n\nNAME                        TYPE           CLUSTER-IP        EXTERNAL-IP                     PORT(S)          AGE\nservice/hazelcast-service   LoadBalancer   x.x.x.x             y.y.y.y                       5701:32567/TCP   1m\n\nNAME                         DESIRED   CURRENT   AGE\nstatefulset.apps/hazelcast   3         3         1m\n```\n\nThis cluster contains 3 replicas, look in one of the pods logs to check if all members are correctly detected. Look for similar messages:\n\n```\n$ oc get pods\n\nMembers {size:3, ver:7} [\n\tMember [192.168.16.15]:5701 - 9dc4af56-6df8-4a65-9890-68bf99e0ea5a\n\tMember [192.168.12.16]:5701 - f927d37c-860d-4b38-88b4-b1f3dd97bfd1 this\n\tMember [192.168.22.15]:5701 - ec8602a7-59b0-4dab-8446-3a8f32dac845\n]\n```\n\n#### Deploy OpenLiberty configured for Hazelcast\nThe Hazelcast client configured with OpenLiberty is using Kubernetes API to find Hazelcast cluster and requires a new ServiceAccount and ClusterRoleBinding\n\nCreate project for the client application:\n\n`$ oc new-project appmod-hazelcast-liberty`\n\nCreate a new ServiceAccount:\n\n`$ oc create serviceaccount hazelcast-liberty -n appmod-hazelcast-liberty`\n\nCreate rbac.yaml with the following content:\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: default-cluster\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: view\nsubjects:\n- kind: ServiceAccount\n  name: hazelcast-liberty\n  namespace: appmod-hazelcast-liberty\n\n```\n\nThen, apply `rbac.yaml`:\n\n```bash\n$ kubectl apply -f rbac.yaml\n```\n\n*Note*: You can be even more strict with the permissions and create your own Role. For details, please check the implementation of [Hazelcast Helm Chart](https://github.com/helm/charts/tree/master/stable/hazelcast).\n\nThe following changes are required to a WebSphere Liberty deployment on OpenShift in order to use Hazelcast:\n\n- an environment variable: `KUBERNETES_NAMESPACE` that is set to the namespace that Hazelcast is deployed in to\n- the deployment much be updated to use the new serviceAccount by adding  the following to the `template/spec` section:\n\n```\nserviceAccountName: hazelcast-liberty\nserviceAccount: hazelcast-liberty\n```\n\nIn order to validate that the in-memory session cache is working a similar test to the session affinity test is executed. There are two pods running the application:\n\n```\noc get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nsession-hazel3-2-425tr   1/1     Running   0          9d\nsession-hazel3-2-p5gtd   1/1     Running   0          9d\n```\n\nOn the first request shown below, the session is empty and the pod is `session-hazel3-2-425tr`\n\n![app1](images/sessions-run/hazel-oc-app1.jpg)\n\nAfter a few requests to the app, the same pod is being used and the session is being retained\n\n![app2](images/sessions-run/hazel-oc-app2.jpg)\n\nIf the pod is then deleted it will be replaced by OpenShift:\n\n```\noc delete pod session-hazel3-2-425tr\npod \"session-hazel3-2-425tr\" deleted\n\noc get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nsession-hazel3-2-p5gtd   1/1     Running   0          9d\nsession-hazel3-2-wc4wd   1/1     Running   0          18s\n```\n\nIf an attempt is then made to refresh the browser page or increment the count then the request is routed to the different pod (`session-hazel3-2--wc4wd`) but the session state is retained:\n\n![app4](images/sessions-run/hazel-oc-app3.jpg)\n\n\n## Session persistence using database\n\nIn some cases you may want to use database instead of cache for storing session data, especially if you earlier already were using database as session persistent layer.\n\nWebSphere Liberty fully supports session persistence using data base using `sessionDatabase-1.0` feature. You will utilize dynamic configuration support in Liberty to configure session persistence without modifying your original server configuration.\n\n### Liberty configuration\n\nCreate new configuration file `session-db.xml` with the following contents. It configures required feature, database driver libraries, datasource, and http session management.\n\n```xml\n<server description=\"Demonstrates HTTP Session Persistence Configuration\">\n  <featureManager>\n          <feature>sessionDatabase-1.0</feature>\n  </featureManager>\n\n  <library id=\"DB2Lib\">\n    <fileset dir=\"${server.config.dir}/resources/db2\" includes=\"*.jar\"/>\n  </library>\n\n  <dataSource id=\"SessionsDS\">\n    <jdbcDriver libraryRef=\"DB2Lib\"/>\n    <properties.db2.jcc databaseName=\"${env.DB2_DBNAME}\" password=\"${env.DB2_PASSWORD}\" portNumber=\"${env.DB2_PORT}\" serverName=\"${env.DB2_HOST}\" user=\"${env.DB2_USER}\"/>\n    <connectionManager agedTimeout=\"0\" connectionTimeout=\"180\" maxIdleTime=\"1800\" maxPoolSize=\"10\" minPoolSize=\"1\" reapTime=\"180\"/>\n  </dataSource>\n\n  <httpSessionDatabase id=\"SessionDB\" dataSourceRef=\"SessionsDS\"/>\n  <httpSession cloneId=\"${env.HOSTNAME}\"/>\n\n</server>\n\n```\n\nIn the configuration you are utilizing environment variables for database access parameters such as host, port, userid, etc (e.g. `${env.DB2_HOST}`). These variables will be provided during the deployment.\n\n### Modifying the Dockerfile\n\nAs now solution requires additional configuration file and database driver to access session database, that needs to be added to the Dockerfile.\n\n```\n# session persistence\nCOPY --chown=1001:0 db2drivers/ /config/resources/db2\nCOPY --chown=1001:0 src/main/liberty/config/session-db.xml /config/configDropins/overrides/\n```\n\n### Deploying to OpenShift\nThe only changes required to an existing WebSphere Liberty deployment are to add the Environment Variables for the DB2 database that is being used as the session database:\n\n- Environment Variables: `DB2_HOST`, `DB2_PORT`, `DB2_DBNAME`, `DB2_USER` and `DB2_PASSWORD` should be added to the deployment.\n\nThe same test that has been used for *session affinity* and *in-memory session cache* can be used to validate *session persistence using a database*.\n\n## Summary\nIn this article we have demonstrated how to use session affinity, session caching using an in-memory cache and session persistence using a database with WebSphere Liberty on OpenShift.\n","type":"Mdx","contentDigest":"7982428195e70757b76db4e432effd97","counter":449,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Managing session state","description":"Best practices and techniques to managing session state in an application when modernizing"},"exports":{},"rawBody":"---\ntitle: Managing session state\ndescription: Best practices and techniques to managing session state in an application when modernizing\n---\n\nexport const Title = () => (\n  <span>\nModernizing runtimes with Liberty <br/> <h2>Managing session state in an application</h2>\n  </span>\n);\n\n<PageDescription>\nBest practices and techniques to managing session state in an application when modernizing\n</PageDescription>\n\n\n\n\n# Session state management considerations\n\nOne of the [12 factor](https://12factor.net/) principles is that applications and services should be stateless. Many existing monolith applications rely heavily on HTTP sessions. When you are modernizing your application it is a good practice to rearchitect them to be stateless, but some times it is not an easy task and you need to postpone it for a while.\n\nThere are following methods that you can use in container world to handle session state, depending on your requirements:\n- session affinity\n- session replication using caching mechanisms\n- session persistence using database\n\n\n## Session affinity\n\nSession affinity is the simplest mechanism for maintaining session state. Using this mechanism, requests from the same session will be routed to the same POD. Selecting this solution, you have to be aware that PODs are not like on-premise servers, they may be restarted much often, in many circumstances. So this solution might be applicable to applications that need relatively short lived sessions, and can tolerate loosing session data.\n\n### Deploying with session affinity\n\nSession affinity on OpenShift utilizes Route configuration. You don't have to make any changes in your application code or during building application container, all is done on deployment time.\n\nIn this example, a simple application has been deployed to WebSphere Liberty that displays the contents of the *session* and allows updates to be made to a *counter* that is stored in the session as well as displaying the name of the *pod* that services the request. There are multiple instances of the application pod running:\n\n```\noc get pods | grep sample\nsession-sample-1-bzpd7   1/1     Running   0          10d\nsession-sample-1-dnpsg   1/1     Running   0          10d\nsession-sample-1-g5c8k   1/1     Running   0          21d\n```\n\nOn the first request, the output is as shown below with the name of the pod an empty session. Since this is first request, currently there is no data in the session. You can also see name of the pod that runs the application.\n\n![app1](images/sessions-run/oc-app1.jpg)\n\nAfter a few requests to the app, using the link or reloading the page, the the counter value that is taken from the session increases as shwon below. Note that the requests are routed to the same pod. Session affinity is working.\n\n![app2](images/sessions-run/oc-app2.jpg)\n\nIf the pod is then deleted it will be replaced by OpenShift:\n\n```\noc delete pod session-sample-1-g5c8k\npod \"session-sample-1-g5c8k\" deleted\n\noc get pods | grep sample\nsession-sample-1-5hzd5   1/1     Running   0          3m\nsession-sample-1-7w8gh   1/1     Running   0          23s\nsession-sample-1-dnpsg   1/1     Running   0          10d\n```\n\nIf an attempt is then made to refresh the browser page or increment the count then the request is routed to the different pod and session state is lost:\n\n![app4](images/sessions-run/oc-app3.jpg)\n\nIf you want to ensure that session state is not lost when pod is removed, you will need to configure session caching or session persistence.\n\n## Session replication using caching mechanisms\nVery often session affinity is not enough for application requirements and you need session state to be preserved in case of pod failure. One of the mechanisms that could be used is *session caching*. WebSphere Liberty has a `sessionCache-1.0` feature which provides distributed in-memory HttpSession caching. The `sessionCache-1.0` feature builds on top of an existing technology called [JCache (JSR 107)](https://www.jcp.org/en/jsr/detail?id=107), which offers a standardized distributed in-memory caching API.\n\nThe `sessionCache-1.0` feature does not include a JCache implementation, so you need to pick one and reference it as a `<library>` in your `server.xml`. WebSphere/Open Liberty supports the following JCache implementations:\n- Hazelcast\n- WebSphere Extreme Scale\n- Infinispan\n- Ehcache\n\n### Prepare DockerFile with caching configuration\nThis article shows how to enable [Hazelcast In-Memory Data Grid](https://hazelcast.org/), as it is easily available in many private cloud solutions.\n\nEnabling Hazelcast session caching retrieves the Hazelcast client libraries from the [hazelcast/hazelcast](https://hub.docker.com/r/hazelcast/hazelcast/) Docker image, configures Hazelcast by copying a sample hazelcast.xml, and configures the Liberty server feature sessionCache-1.0 by including the XML snippet hazelcast-sessioncache.xml. By default, the Hazelcast Discovery Plugin for Kubernetes will auto-discover its peers within the same Kubernetes namespace. To enable this functionality, the Docker image author can include the following Dockerfile snippet, and choose from either client-server or embedded topology.\n\nModify your current Dockerfile with the following lines:\n\n```dockerfile\n### Hazelcast Session Caching ###\n# Copy the Hazelcast libraries from the Hazelcast Docker image - paths for WebSphere Liberty\nCOPY --from=hazelcast/hazelcast --chown=1001:0 /opt/hazelcast/lib/*.jar /opt/ibm/wlp/usr/shared/resources/hazelcast/\n\n# Copy the Hazelcast libraries from the Hazelcast Docker image - paths for Open Liberty\n# COPY --from=hazelcast/hazelcast --chown=1001:0 /opt/hazelcast/lib/*.jar /opt/ol/wlp/usr/shared/resources/hazelcast/\n\n# Instruct configure.sh to copy the client topology hazelcast.xml\nARG HZ_SESSION_CACHE=client\n\n# Instruct configure.sh to copy the embedded topology hazelcast.xml and set the required system property\n#ARG HZ_SESSION_CACHE=embedded\n#ENV JAVA_TOOL_OPTIONS=\"-Dhazelcast.jcache.provider.type=server ${JAVA_TOOL_OPTIONS}\"\n\n## This script will add the requested XML snippets and grow image to be fit-for-purpose\nRUN configure.sh\n```\n\n### Deploy Hazelcast in OpenShift\nHazelcast can be used in OpenShift. By default it requires paid version - Hazelcast Enterprise. But you can use also free version [Hazelcast OpenShift Origin](https://github.com/hazelcast/hazelcast-code-samples/blob/master/hazelcast-integration/openshift/hazelcast-cluster/hazelcast-openshift-origin). See details on this page [Hazelcast for OpenShift](https://github.com/hazelcast/hazelcast-code-samples/tree/master/hazelcast-integration/openshift)\n\nFor this article you will use Hazelcast OpenShift Origin.\n\n#### Deploy Hazelcast cluster\nEasiest way to deploy Hazelcast is to use [hazelcast.yaml](https://github.com/hazelcast/hazelcast-code-samples/blob/master/hazelcast-integration/openshift/hazelcast-cluster/hazelcast-openshift-origin/hazelcast.yaml) file provided by Hazelcast\n\nCreate project for Hazelcast cluster\n\n`$ oc new-project appmod-hazelcast`\n\nDeploy Hazelcast cluster\n\n`$ oc new-app -f hazelcast.yaml -p NAMESPACE=appmod-hazelcast`\n\nCheck the status of deployed cluster\n\n```\n$ oc get all\nNAME              READY     STATUS              RESTARTS   AGE\npod/hazelcast-0   1/1       Running             0          1m\npod/hazelcast-1   1/1       Running             0          42s\npod/hazelcast-2   0/1       ContainerCreating   0          2s\n\nNAME                        TYPE           CLUSTER-IP        EXTERNAL-IP                     PORT(S)          AGE\nservice/hazelcast-service   LoadBalancer   x.x.x.x             y.y.y.y                       5701:32567/TCP   1m\n\nNAME                         DESIRED   CURRENT   AGE\nstatefulset.apps/hazelcast   3         3         1m\n```\n\nThis cluster contains 3 replicas, look in one of the pods logs to check if all members are correctly detected. Look for similar messages:\n\n```\n$ oc get pods\n\nMembers {size:3, ver:7} [\n\tMember [192.168.16.15]:5701 - 9dc4af56-6df8-4a65-9890-68bf99e0ea5a\n\tMember [192.168.12.16]:5701 - f927d37c-860d-4b38-88b4-b1f3dd97bfd1 this\n\tMember [192.168.22.15]:5701 - ec8602a7-59b0-4dab-8446-3a8f32dac845\n]\n```\n\n#### Deploy OpenLiberty configured for Hazelcast\nThe Hazelcast client configured with OpenLiberty is using Kubernetes API to find Hazelcast cluster and requires a new ServiceAccount and ClusterRoleBinding\n\nCreate project for the client application:\n\n`$ oc new-project appmod-hazelcast-liberty`\n\nCreate a new ServiceAccount:\n\n`$ oc create serviceaccount hazelcast-liberty -n appmod-hazelcast-liberty`\n\nCreate rbac.yaml with the following content:\n\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: default-cluster\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: view\nsubjects:\n- kind: ServiceAccount\n  name: hazelcast-liberty\n  namespace: appmod-hazelcast-liberty\n\n```\n\nThen, apply `rbac.yaml`:\n\n```bash\n$ kubectl apply -f rbac.yaml\n```\n\n*Note*: You can be even more strict with the permissions and create your own Role. For details, please check the implementation of [Hazelcast Helm Chart](https://github.com/helm/charts/tree/master/stable/hazelcast).\n\nThe following changes are required to a WebSphere Liberty deployment on OpenShift in order to use Hazelcast:\n\n- an environment variable: `KUBERNETES_NAMESPACE` that is set to the namespace that Hazelcast is deployed in to\n- the deployment much be updated to use the new serviceAccount by adding  the following to the `template/spec` section:\n\n```\nserviceAccountName: hazelcast-liberty\nserviceAccount: hazelcast-liberty\n```\n\nIn order to validate that the in-memory session cache is working a similar test to the session affinity test is executed. There are two pods running the application:\n\n```\noc get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nsession-hazel3-2-425tr   1/1     Running   0          9d\nsession-hazel3-2-p5gtd   1/1     Running   0          9d\n```\n\nOn the first request shown below, the session is empty and the pod is `session-hazel3-2-425tr`\n\n![app1](images/sessions-run/hazel-oc-app1.jpg)\n\nAfter a few requests to the app, the same pod is being used and the session is being retained\n\n![app2](images/sessions-run/hazel-oc-app2.jpg)\n\nIf the pod is then deleted it will be replaced by OpenShift:\n\n```\noc delete pod session-hazel3-2-425tr\npod \"session-hazel3-2-425tr\" deleted\n\noc get pods\nNAME                     READY   STATUS    RESTARTS   AGE\nsession-hazel3-2-p5gtd   1/1     Running   0          9d\nsession-hazel3-2-wc4wd   1/1     Running   0          18s\n```\n\nIf an attempt is then made to refresh the browser page or increment the count then the request is routed to the different pod (`session-hazel3-2--wc4wd`) but the session state is retained:\n\n![app4](images/sessions-run/hazel-oc-app3.jpg)\n\n\n## Session persistence using database\n\nIn some cases you may want to use database instead of cache for storing session data, especially if you earlier already were using database as session persistent layer.\n\nWebSphere Liberty fully supports session persistence using data base using `sessionDatabase-1.0` feature. You will utilize dynamic configuration support in Liberty to configure session persistence without modifying your original server configuration.\n\n### Liberty configuration\n\nCreate new configuration file `session-db.xml` with the following contents. It configures required feature, database driver libraries, datasource, and http session management.\n\n```xml\n<server description=\"Demonstrates HTTP Session Persistence Configuration\">\n  <featureManager>\n          <feature>sessionDatabase-1.0</feature>\n  </featureManager>\n\n  <library id=\"DB2Lib\">\n    <fileset dir=\"${server.config.dir}/resources/db2\" includes=\"*.jar\"/>\n  </library>\n\n  <dataSource id=\"SessionsDS\">\n    <jdbcDriver libraryRef=\"DB2Lib\"/>\n    <properties.db2.jcc databaseName=\"${env.DB2_DBNAME}\" password=\"${env.DB2_PASSWORD}\" portNumber=\"${env.DB2_PORT}\" serverName=\"${env.DB2_HOST}\" user=\"${env.DB2_USER}\"/>\n    <connectionManager agedTimeout=\"0\" connectionTimeout=\"180\" maxIdleTime=\"1800\" maxPoolSize=\"10\" minPoolSize=\"1\" reapTime=\"180\"/>\n  </dataSource>\n\n  <httpSessionDatabase id=\"SessionDB\" dataSourceRef=\"SessionsDS\"/>\n  <httpSession cloneId=\"${env.HOSTNAME}\"/>\n\n</server>\n\n```\n\nIn the configuration you are utilizing environment variables for database access parameters such as host, port, userid, etc (e.g. `${env.DB2_HOST}`). These variables will be provided during the deployment.\n\n### Modifying the Dockerfile\n\nAs now solution requires additional configuration file and database driver to access session database, that needs to be added to the Dockerfile.\n\n```\n# session persistence\nCOPY --chown=1001:0 db2drivers/ /config/resources/db2\nCOPY --chown=1001:0 src/main/liberty/config/session-db.xml /config/configDropins/overrides/\n```\n\n### Deploying to OpenShift\nThe only changes required to an existing WebSphere Liberty deployment are to add the Environment Variables for the DB2 database that is being used as the session database:\n\n- Environment Variables: `DB2_HOST`, `DB2_PORT`, `DB2_DBNAME`, `DB2_USER` and `DB2_PASSWORD` should be added to the deployment.\n\nThe same test that has been used for *session affinity* and *in-memory session cache* can be used to validate *session persistence using a database*.\n\n## Summary\nIn this article we have demonstrated how to use session affinity, session caching using an in-memory cache and session persistence using a database with WebSphere Liberty on OpenShift.\n","fileAbsolutePath":"/home/runner/work/modernization-playbook/modernization-playbook/src/pages/applications/liberty/liberty-sessions.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}